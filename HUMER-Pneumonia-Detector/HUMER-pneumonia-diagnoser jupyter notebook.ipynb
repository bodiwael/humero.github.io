{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b84e1b73e798e65d1b6b2e24b37b6652bfdc5dc"
   },
   "source": [
    "**Introduction**\n",
    "\n",
    "HUMERO (Humanoid Medical Robot) is a medical robot assistant, that used in Pneumonia duagoniser.\n",
    "It is based on high technical Artificial intelligence and machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9176cf20437ce97ec4de7fc1da1e38dd98c9d0e6"
   },
   "source": [
    "### Check how many files in each folder\n",
    "Note: In some folders there is a non image file called  '.DS_Store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20610b62a188caae8feb100b640747e121760be1"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "# normal\n",
    "print(len(os.listdir('../input/chest_xray/chest_xray/train/NORMAL')))\n",
    "# pneumonia\n",
    "print(len(os.listdir('../input/chest_xray/chest_xray/train/PNEUMONIA')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69fc9a807e308483fd4df7dca0fadc459800ea6d"
   },
   "outputs": [],
   "source": [
    "# Val\n",
    "# normal\n",
    "print(len(os.listdir('../input/chest_xray/chest_xray/val/NORMAL')))\n",
    "# pneumonia\n",
    "print(len(os.listdir('../input/chest_xray/chest_xray/val/PNEUMONIA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "018ae86b3184c04d4bf7bf78c7e32668259a2ece"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "# normal\n",
    "print(len(os.listdir('../input/chest_xray/chest_xray/test/NORMAL')))\n",
    "# pneumonia\n",
    "print(len(os.listdir('../input/chest_xray/chest_xray/test/PNEUMONIA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63e77a0ff1dd143c948053d2e3c0938ee6568c2e"
   },
   "outputs": [],
   "source": [
    "os.listdir('../input/chest_xray/chest_xray/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29f3a90bb237db373b2adacc5d8904224ddfa4db"
   },
   "source": [
    "### Create a Dataframe containing all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1606ff5100a5c6e5845f72a12ea71f611555c082"
   },
   "outputs": [],
   "source": [
    "# create a list of files in each folder\n",
    "train_normal_list = os.listdir('../input/chest_xray/chest_xray/train/NORMAL')\n",
    "train_pneu_list = os.listdir('../input/chest_xray/chest_xray/train/PNEUMONIA')\n",
    "val_normal_list = os.listdir('../input/chest_xray/chest_xray/val/NORMAL')\n",
    "val_pneu_list = os.listdir('../input/chest_xray/chest_xray/val/PNEUMONIA')\n",
    "test_normal_list = os.listdir('../input/chest_xray/chest_xray/test/NORMAL')\n",
    "test_pneu_list = os.listdir('../input/chest_xray/chest_xray/test/PNEUMONIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1cf8b0162f8398c4587a631b136fd19cbade0db0"
   },
   "outputs": [],
   "source": [
    "def assign_pneu_type(x):\n",
    "    x = str(x)\n",
    "    if 'virus' in x:\n",
    "        return 'viral'\n",
    "    if 'bacteria' in x:\n",
    "        return 'bacterial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae3356a07b03ceaa89c006f22161095472ee9423"
   },
   "outputs": [],
   "source": [
    "# TRAIN_NORMAL\n",
    "# create the dataframe\n",
    "df_train_normal = pd.DataFrame(train_normal_list, columns=['image_id'])\n",
    "# delete the entry named .DS_Store\n",
    "df_train_normal = df_train_normal[df_train_normal['image_id'] != '.DS_Store']\n",
    "# create a new target column\n",
    "df_train_normal['target'] = 'normal'\n",
    "\n",
    "# TRAIN_PNEU\n",
    "# create the dataframe\n",
    "df_train_pneu = pd.DataFrame(train_pneu_list, columns=['image_id'])\n",
    "# delete the entry named .DS_Store\n",
    "df_train_pneu = df_train_pneu[df_train_pneu['image_id'] != '.DS_Store']\n",
    "# create a target column that's a copy of the image column\n",
    "df_train_pneu['target'] = df_train_pneu['image_id']\n",
    "# apply the function to this target column\n",
    "df_train_pneu['target'] = df_train_pneu['target'].apply(assign_pneu_type)\n",
    "\n",
    "# VAL_NORMAL\n",
    "# create the dataframe\n",
    "df_val_normal = pd.DataFrame(val_normal_list, columns=['image_id'])\n",
    "# delete the entry named .DS_Store\n",
    "df_val_normal = df_val_normal[df_val_normal['image_id'] != '.DS_Store']\n",
    "# create a new target column\n",
    "df_val_normal['target'] = 'normal'\n",
    "\n",
    "\n",
    "# VAL_PNEU\n",
    "# create the dataframe\n",
    "df_val_pneu = pd.DataFrame(val_pneu_list, columns=['image_id'])\n",
    "# delete the entry named .DS_Store\n",
    "df_val_pneu = df_val_pneu[df_val_pneu['image_id'] != '.DS_Store']\n",
    "# create a target column that's a copy of the image column\n",
    "df_val_pneu['target'] = df_val_pneu['image_id']\n",
    "# apply the function to this target column\n",
    "df_val_pneu['target'] = df_val_pneu['target'].apply(assign_pneu_type)\n",
    "\n",
    "\n",
    "# TEST_NORMAL\n",
    "# create the dataframe\n",
    "df_test_normal = pd.DataFrame(test_normal_list, columns=['image_id'])\n",
    "# delete the entry named .DS_Store\n",
    "df_test_normal = df_test_normal[df_test_normal['image_id'] != '.DS_Store']\n",
    "# create a new target column\n",
    "df_test_normal['target'] = 'normal'\n",
    "\n",
    "\n",
    "# TEST_PNEU\n",
    "# create the dataframe\n",
    "df_test_pneu = pd.DataFrame(test_pneu_list, columns=['image_id'])\n",
    "# delete the entry named .DS_Store\n",
    "df_test_pneu = df_test_pneu[df_test_pneu['image_id'] != '.DS_Store']\n",
    "# create a target column that's a copy of the image column\n",
    "df_test_pneu['target'] = df_test_pneu['image_id']\n",
    "# apply the function to this target column\n",
    "df_test_pneu['target'] = df_test_pneu['target'].apply(assign_pneu_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0668aea9a1ec52a69c8b9163f2768a2aef006b2f"
   },
   "outputs": [],
   "source": [
    "# Concat the dataframes\n",
    "df_data = \\\n",
    "pd.concat([df_train_normal,df_train_pneu,df_val_normal,df_val_pneu,df_test_normal,\n",
    "           df_test_pneu],axis=0).reset_index(drop=True)\n",
    "\n",
    "# shuffle\n",
    "df_data = shuffle(df_data)\n",
    "\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a46826407b4747645877db0f441743af01a813ee"
   },
   "outputs": [],
   "source": [
    "# Check the target distribution\n",
    "df_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76fdaa421248d16625fa70de503a3c2b2e03b65e"
   },
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "082b377f07e751a494f11b4152289f8ffa4bfe9c"
   },
   "source": [
    "### Create the directory structure\n",
    "The original dataset directory structure was set up to support a binary classification problem. Because we will be predicting 3 classes we need to change this structure. In these folders we will store the images that will later be fed to the Keras generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2810100a4ec696669960325015177285f51673e0"
   },
   "outputs": [],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "\n",
    "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
    "\n",
    "# now we create 3 folders inside 'base_dir':\n",
    "\n",
    "# train\n",
    "    # normal\n",
    "    # bacterial\n",
    "    # viral\n",
    "\n",
    "# val\n",
    "    # normal\n",
    "    # bacterial\n",
    "    # viral\n",
    "\n",
    "\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "\n",
    "# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n",
    "# Inside each folder we create seperate folders for each class\n",
    "\n",
    "# create new folders inside train_dir\n",
    "normal = os.path.join(train_dir, 'normal')\n",
    "os.mkdir(normal)\n",
    "bacterial = os.path.join(train_dir, 'bacterial')\n",
    "os.mkdir(bacterial)\n",
    "viral = os.path.join(train_dir, 'viral')\n",
    "os.mkdir(viral)\n",
    "\n",
    "\n",
    "\n",
    "# create new folders inside val_dir\n",
    "normal = os.path.join(val_dir, 'normal')\n",
    "os.mkdir(normal)\n",
    "bacterial = os.path.join(val_dir, 'bacterial')\n",
    "os.mkdir(bacterial)\n",
    "viral = os.path.join(val_dir, 'viral')\n",
    "os.mkdir(viral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a0633db439a16d57c8b852f847b9aab858e9d66"
   },
   "outputs": [],
   "source": [
    "os.listdir('base_dir/train_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d6d535b71118a8b853a57dc1fe9d72853823574"
   },
   "source": [
    "### Create Train and Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ade7b091c68d4e7c5ca4a38af9d88bec5242a643"
   },
   "outputs": [],
   "source": [
    "y = df_data['target']\n",
    "\n",
    "df_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49d030823b3867f9152b43ef3533e2b3377551ec"
   },
   "outputs": [],
   "source": [
    "# check df_train class distribution\n",
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "750f7c95d68049fddd8762f53450e30ccc840e33"
   },
   "outputs": [],
   "source": [
    "# check df_val class distribution\n",
    "df_val['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8216a799846f225000273149eaa71c85145d505"
   },
   "source": [
    "### Transfer the Images into the Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d1235a2868a417957be682cab82194d5058418f"
   },
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b217b454606ac413dbb6c694c4e789b4eb3a2877"
   },
   "outputs": [],
   "source": [
    "# Set the image_id as the index in df_data\n",
    "df_data.set_index('image_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "413a7abf6b0f5bde099b31d3f47e5e66854f2604"
   },
   "outputs": [],
   "source": [
    "# Get a list of images in each of the folders\n",
    "train_normal_list = os.listdir('../input/chest_xray/chest_xray/train/NORMAL')\n",
    "train_pneu_list = os.listdir('../input/chest_xray/chest_xray/train/PNEUMONIA')\n",
    "val_normal_list = os.listdir('../input/chest_xray/chest_xray/val/NORMAL')\n",
    "val_pneu_list = os.listdir('../input/chest_xray/chest_xray/val/PNEUMONIA')\n",
    "test_normal_list = os.listdir('../input/chest_xray/chest_xray/test/NORMAL')\n",
    "test_pneu_list = os.listdir('../input/chest_xray/chest_xray/test/PNEUMONIA')\n",
    "\n",
    "# Get a list of train and val images\n",
    "train_list = list(df_train['image_id'])\n",
    "val_list = list(df_val['image_id'])\n",
    "\n",
    "# Transfer the train images\n",
    "\n",
    "for image in train_list:\n",
    "    \n",
    "    fname = image\n",
    "    label = df_data.loc[image,'target']\n",
    "    \n",
    "    if fname in train_normal_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/train/NORMAL', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    if fname in train_pneu_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/train/PNEUMONIA', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    if fname in val_normal_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/val/NORMAL', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    if fname in val_pneu_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/val/PNEUMONIA', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    if fname in test_normal_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/test/NORMAL', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    if fname in test_pneu_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/test/PNEUMONIA', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Transfer the val images\n",
    "\n",
    "for image in val_list:\n",
    "    \n",
    "    fname = image\n",
    "    label = df_data.loc[image,'target']\n",
    "    \n",
    "    if fname in train_normal_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/train/NORMAL', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    if fname in train_pneu_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/train/PNEUMONIA', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    if fname in val_normal_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/val/NORMAL', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    if fname in val_pneu_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/val/PNEUMONIA', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    if fname in test_normal_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/test/NORMAL', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    if fname in test_pneu_list:\n",
    "        # source path to image\n",
    "        src = os.path.join('../input/chest_xray/chest_xray/test/PNEUMONIA', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94c75a3e78729246eca083a74ee008b30595a823"
   },
   "outputs": [],
   "source": [
    "# check how many train images we have in each folder\n",
    "\n",
    "print(len(os.listdir('base_dir/train_dir/normal')))\n",
    "print(len(os.listdir('base_dir/train_dir/bacterial')))\n",
    "print(len(os.listdir('base_dir/train_dir/viral')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c989df54967bc3f0d03c022cdf6dd6ec09aa237b"
   },
   "outputs": [],
   "source": [
    "# check how many val images we have in each folder\n",
    "\n",
    "print(len(os.listdir('base_dir/val_dir/normal')))\n",
    "print(len(os.listdir('base_dir/val_dir/bacterial')))\n",
    "print(len(os.listdir('base_dir/val_dir/viral')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76f7e2deb4c9795172a670b1608f185cf71b17a0"
   },
   "source": [
    "### Copy the train images into aug_dir\n",
    "aug_dir is where we temporarily store images from a given class before feeding them into the generator for augmentation.¶ \n",
    "\n",
    "We will not be augmenting on the fly. We will create augmented images, store them in folders together with the raw images and then feed these into the generators. I found that working this way makes the training process run faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19039caec66c67fe71422d319c579a167f35a72d"
   },
   "outputs": [],
   "source": [
    "\n",
    "class_list = ['normal','bacterial','viral']\n",
    "\n",
    "for item in class_list:\n",
    "    \n",
    "    # We are creating temporary directories here because we delete these directories later\n",
    "    # create a base dir\n",
    "    aug_dir = 'aug_dir'\n",
    "    os.mkdir(aug_dir)\n",
    "    # create a dir within the base dir to store images of the same class\n",
    "    img_dir = os.path.join(aug_dir, 'img_dir')\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "    # Choose a class\n",
    "    img_class = item\n",
    "\n",
    "    # list all images in that directory\n",
    "    img_list = os.listdir('base_dir/train_dir/' + img_class)\n",
    "\n",
    "    # Copy images from the class train dir to the img_dir e.g. class 'bacterial'\n",
    "    for fname in img_list:\n",
    "            # source path to image\n",
    "            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n",
    "            # destination path to image\n",
    "            dst = os.path.join(img_dir, fname)\n",
    "            # copy the image from the source to the destination\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "    # point to a dir containing the images and not to the images themselves\n",
    "    path = aug_dir\n",
    "    save_path = 'base_dir/train_dir/' + img_class\n",
    "\n",
    "    # Create a data generator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        zoom_range=0.05,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    aug_datagen = datagen.flow_from_directory(path,\n",
    "                                           save_to_dir=save_path,\n",
    "                                           save_format='jpg',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    # Generate the augmented images and add them to the training folders\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    num_aug_images_wanted = 5000 # total number of images we want to have in each class\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    num_files = len(os.listdir(img_dir))\n",
    "    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n",
    "\n",
    "    # run the generator and create augmented images\n",
    "    for i in range(0,num_batches):\n",
    "\n",
    "        imgs, labels = next(aug_datagen)\n",
    "        \n",
    "    # delete temporary directory with the raw image files\n",
    "    shutil.rmtree('aug_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bbd90acf7710878c09c9f104df47a29c2fc9a241"
   },
   "outputs": [],
   "source": [
    "# Check how many train images we now have in each folder.\n",
    "# This is the original images plus the augmented images.\n",
    "\n",
    "print(len(os.listdir('base_dir/train_dir/normal')))\n",
    "print(len(os.listdir('base_dir/train_dir/bacterial')))\n",
    "print(len(os.listdir('base_dir/train_dir/viral')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e49216abd923035bb77b1f9ed91c4c09e355a1c6"
   },
   "outputs": [],
   "source": [
    "# Check how many val images we have in each folder.\n",
    "\n",
    "print(len(os.listdir('base_dir/val_dir/normal')))\n",
    "print(len(os.listdir('base_dir/val_dir/bacterial')))\n",
    "print(len(os.listdir('base_dir/val_dir/viral')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d81d5ae06580b221c970803401454b81d3fee3b"
   },
   "source": [
    "### Visualize 50 augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45f8d997f661cf5d3cb57b94e96bcfa168081b03"
   },
   "outputs": [],
   "source": [
    "# plots images with labels within jupyter notebook\n",
    "# source: https://github.com/smileservices/keras_utils/blob/master/utils.py\n",
    "\n",
    "def plots(ims, figsize=(12,6), rows=5, interp=False, titles=None): # 12,6\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "        \n",
    "plots(imgs, titles=None) # titles=labels will display the image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26dff2ebe9841d2a245fec5d3c5819e8577b2da0"
   },
   "outputs": [],
   "source": [
    "# End of Data Preparation\n",
    "### ===================================================================================== ###\n",
    "# Start of Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65ad7c603039d2b932a8745cf29cfff1f756dc73"
   },
   "source": [
    "### Set Up the Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e85947855ec8d9202fe9614c6f6779a98d3f4268"
   },
   "outputs": [],
   "source": [
    "train_path = 'base_dir/train_dir'\n",
    "valid_path = 'base_dir/val_dir'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 10\n",
    "val_batch_size = 10\n",
    "image_size = 224\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a32b6164f7beed92d146fa816f27f453d506c2e7"
   },
   "outputs": [],
   "source": [
    "train_batches = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "                                                    train_path,\n",
    "                                                    target_size=(image_size,image_size),\n",
    "                                                    batch_size=train_batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "valid_batches = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "                                                    valid_path,\n",
    "                                                    target_size=(image_size,image_size),\n",
    "                                                    batch_size=val_batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_batches = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "                                                    valid_path,\n",
    "                                                    target_size=(image_size,image_size),\n",
    "                                                    batch_size=val_batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9402ab1513e71f684af0bd1d1ee73d9f09728604"
   },
   "source": [
    "### Modify MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1641e3d751b54e1cd4b4b8c2cf08b922ad5284b1"
   },
   "outputs": [],
   "source": [
    "# create a copy of a mobilenet model\n",
    "\n",
    "mobile = keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "65c59c856ee825672f5a690180668bcfe5a89c78"
   },
   "outputs": [],
   "source": [
    "mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "636a0b2aa3225b02276322bc2b9c9f13d169d467"
   },
   "outputs": [],
   "source": [
    "type(mobile.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08405602c876e30cb1f07bba7668942981aa7aef"
   },
   "outputs": [],
   "source": [
    "# How many layers does MobileNet have?\n",
    "len(mobile.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d6612a3b90c4986046319d2c6e19b4e28bef875"
   },
   "outputs": [],
   "source": [
    "# CREATE THE MODEL ARCHITECTURE\n",
    "\n",
    "# Exclude the last 5 layers of the above model.\n",
    "# This will include all layers up to and including global_average_pooling2d_1\n",
    "x = mobile.layers[-6].output\n",
    "\n",
    "# Create a new dense layer for predictions\n",
    "# 3 corresponds to the number of classes\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n",
    "# dense layer we created above.\n",
    "\n",
    "model = Model(inputs=mobile.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "4bcd74c8abcccf69294305b7fca4ef5f1f8116c7"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afef69cf27aa5e9b2d3c24ab996628ef759a6247"
   },
   "outputs": [],
   "source": [
    "# We need to choose how many layers we actually want to be trained.\n",
    "\n",
    "# Here we are freezing the weights of all layers except the\n",
    "# last 40 layers in the new model.\n",
    "# The last 40 layers of the model will be trained.\n",
    "\n",
    "for layer in model.layers[:-40]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44d63f5ca471eaa41baee99c6bc2c39724dc5e12"
   },
   "source": [
    "### Train the Model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "188fac870865b9c75ddaae460742d9c20e793719"
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.001), loss='categorical_crossentropy', \n",
    "              metrics=[categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a7211ef9b53ca496a1ce17767eb357f8bb9b50b"
   },
   "outputs": [],
   "source": [
    "# Get the labels that are associated with each index\n",
    "print(valid_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "211c63114f21f96b34f2373cb1972247cf585fc4"
   },
   "outputs": [],
   "source": [
    "# Add weights to try to make the model more sensitive to a specific class\n",
    "\n",
    "class_weights={\n",
    "    0: 1.0, # bacterial\n",
    "    1: 1.0, # normal\n",
    "    2: 1.0, # viral\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "5ac6cfff960d3149b6b40c4b5d1b701539f749bd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                              #class_weight=class_weights,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=30, verbose=1,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "769b703d82e8abf9d9aaa7e712844383319f0116"
   },
   "source": [
    "### Evaluate the model using the val set¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68ab45d7c1f94b7a663d7bfa125d989a9ba10811"
   },
   "outputs": [],
   "source": [
    "# get the metric names so we can use evaulate_generator\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f6ca463f356c89e6d9d208565ec95e19df5c520"
   },
   "outputs": [],
   "source": [
    "# Here the the last epoch will be used.\n",
    "\n",
    "val_loss, val_cat_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=val_steps)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a52a45d149e8954a286393d27bd9f8600435d10"
   },
   "outputs": [],
   "source": [
    "# Here the best epoch will be used.\n",
    "\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "val_loss, val_cat_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=val_steps)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8015354f1c7e52291494926a9da6ea0d3028537a"
   },
   "source": [
    "### Plot the Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "854c91d4de202849721bd41e7641663f608ce073"
   },
   "outputs": [],
   "source": [
    "# display the loss and accuracy curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training cat acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation cat acc')\n",
    "plt.title('Training and validation cat accuracy')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c68bec225be07ad1282d984850c6836cf589d80"
   },
   "source": [
    "### Create a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "764d139a54fea9320b871c8cee602461eae3828f"
   },
   "outputs": [],
   "source": [
    "# Get the labels of the test images.\n",
    "# Note that cats and dogs are in seperate folders therefore\n",
    "# the code below can get the labels depending on the folder the image is in.\n",
    "\n",
    "test_labels = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "94f1b30b1e4109d28c775d3b6044c45af26b3cf2"
   },
   "outputs": [],
   "source": [
    "# We need these to plot the confusion matrix.\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82dfeca3d83d0a9be1b82cdba024ef89294fa299"
   },
   "outputs": [],
   "source": [
    "# Print the label associated with each class\n",
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d6a69774a54ec47e9103b6e5fa9f847a4e72dbf"
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "predictions = model.predict_generator(test_batches, steps=val_steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f028c4322f5ae30c59207ca0ccb50f0823bac11b"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4f9f8a7be83372e2d1f4239ec41ac34f7812b07"
   },
   "outputs": [],
   "source": [
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc85fb7226dca1fa2a80b1160fed10201af94d1e"
   },
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9fba125b2d296f84f0ae18713c4c79ee2a9764b1"
   },
   "outputs": [],
   "source": [
    "# argmax returns the index of the max value in a row\n",
    "cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a920cb28e1f56c7c3815c775399a2c435b19b714"
   },
   "outputs": [],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45f324fdceec9596c54c4c76227d6087ebf2c278"
   },
   "outputs": [],
   "source": [
    "# Define the labels of the class indices. These need to match the \n",
    "# order shown above.\n",
    "cm_plot_labels = ['bacterial', 'normal', 'viral']\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f84b0ec088af83679516b2fcf01b5878ef1969d"
   },
   "source": [
    "### Create a Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b2396f5670a63be7fb27404fcd6d17af7c1072a"
   },
   "outputs": [],
   "source": [
    "# Get the filenames, labels and associated predictions\n",
    "\n",
    "# This outputs the sequence in which the generator processed the test images\n",
    "test_filenames = test_batches.filenames\n",
    "\n",
    "# Get the true labels\n",
    "y_true = test_batches.classes\n",
    "\n",
    "# Get the predicted labels\n",
    "y_pred = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fb2d0185365bf537404862cf449b8b71006db0c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a classification report\n",
    "\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bc59e9dace04722f4343e76daca299022c23496"
   },
   "source": [
    "### Display Some Incorrect Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a3ac3a69ca42f12bc2a77f44a350764d39b820b"
   },
   "source": [
    "There seems to be many instances where the model found it hard to diffrentiate between bacterial pneumonia images and viral pneumonia images. Is this because these images are similar in some way? Lets print some images and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b779e1d3397dea2949554ebc56fee203f67d609b"
   },
   "outputs": [],
   "source": [
    "# Get the filenames, labels and associated predictions\n",
    "\n",
    "test_filenames = test_batches.filenames\n",
    "test_labels = test_batches.classes\n",
    "preds = predictions.argmax(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a54aad3fcbbf8a2320b10934610b6a3591025e91"
   },
   "outputs": [],
   "source": [
    "# check the lengths of these lists\n",
    "print(len(test_filenames))\n",
    "print(len(test_labels))\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2345370cb0de46523dc5234dc83d0141049d64f2"
   },
   "outputs": [],
   "source": [
    "# Put the above into a dataframe\n",
    "pred_dict = {'filenames': test_filenames, 'labels': test_labels, 'predictions': preds}\n",
    "df_preds = pd.DataFrame(pred_dict)\n",
    "\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "351092d664af610109af6d9fc6772969fbe1669d"
   },
   "outputs": [],
   "source": [
    "# get the indices for the labels\n",
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8173f6037c7fb2aac6d04210d694e75d19eda54"
   },
   "outputs": [],
   "source": [
    "# filter out rows where the label was bacterial (0) and the model predicted viral (2)\n",
    "df_1 = df_preds[(df_preds['labels'] == 0) & (df_preds['predictions'] == 2)]\n",
    "\n",
    "# reset the index\n",
    "df_1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e67db75a792e94345472984fad5812dac788ba85"
   },
   "outputs": [],
   "source": [
    "img_0 = val_dir + '/' + df_1.loc[0, 'filenames']\n",
    "img_1 = val_dir + '/' + df_1.loc[1, 'filenames']\n",
    "img_2 = val_dir + '/' + df_1.loc[2, 'filenames']\n",
    "img_3 = val_dir + '/' + df_1.loc[3, 'filenames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd7f199fcfda35e54bf636f227cda4ae030d7394"
   },
   "outputs": [],
   "source": [
    "# These are 4 bacterial pneumonia images that the model mis-classified as viral pneumonia.\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(plt.imread(img_0), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(plt.imread(img_1), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(plt.imread(img_2), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(plt.imread(img_3), cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0217e890501d0c7437f09210f1991cb14925c888"
   },
   "outputs": [],
   "source": [
    "# Now let's print some true viral pneumonia images\n",
    "\n",
    "df_2 = df_preds[(df_preds['labels'] == 2) & (df_preds['predictions'] == 2)]\n",
    "\n",
    "# reset the index\n",
    "df_2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f33c060491aaa1212204276b2fb8f17c51d80890"
   },
   "outputs": [],
   "source": [
    "img_0 = val_dir + '/' + df_2.loc[0, 'filenames']\n",
    "img_1 = val_dir + '/' + df_2.loc[1, 'filenames']\n",
    "img_2 = val_dir + '/' + df_2.loc[2, 'filenames']\n",
    "img_3 = val_dir + '/' + df_2.loc[3, 'filenames']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(plt.imread(img_0), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(plt.imread(img_1), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(plt.imread(img_2), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(plt.imread(img_3), cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efe9e8c0d6ff31cff8feeb1847324051f20fd2c0"
   },
   "source": [
    "I think this is where domain knowledge would be very helpful. A radiologist would be able to look at these two sets of images and tell us what is the similarity between them that the model is detecting. \n",
    "\n",
    "*In bacterial pneumonia, there will likely be a much more visible presence of fluid in the lungs than viral pneumonia.* Is the model predicting bacterial instead of viral because it's seeing signs of fluid or is something else confusing it? It's a mystery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "601f1a84b15390f5b8b43a2f10f861fcb31d14ea"
   },
   "outputs": [],
   "source": [
    "# End of Model Building\n",
    "### ===================================================================================== ###\n",
    "# Convert the Model from Keras to Tensorflow.js\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88252d9829265e8884b6e38a2eec9c7cc02b4ae8"
   },
   "source": [
    "### Install Tensorflow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "d8d6a3e31b63565a2c532ad458a8b595b3bb2416"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9ce1b2dd5b1fe739728d32f633b78dcd58bb67a"
   },
   "source": [
    "### Convert the model\n",
    "Note: Do not load a saved model and try to convert it. It will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "643e336359480c581d674a91a3f7b818397e82f2"
   },
   "outputs": [],
   "source": [
    "# create a directory to store the model files\n",
    "os.mkdir('tfjs_dir')\n",
    "\n",
    "# convert to Tensorflow.js\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "tfjs.converters.save_keras_model(model, 'tfjs_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0d6bfbddfbb5804043005e497b321ab59f62936"
   },
   "outputs": [],
   "source": [
    "# check the the directory containing the model is available\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f87da1d08a2c464c82c5df791d40b58669eec4db"
   },
   "outputs": [],
   "source": [
    "# view the files that make up the tensorflow.js model\n",
    "os.listdir('tfjs_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7aeaf8c0ba17ec6cf1e9912cd7ff6400e12675b"
   },
   "outputs": [],
   "source": [
    "# Delete the image data directory we created to prevent a Kaggle error.\n",
    "# Kaggle allows a max of 500 files to be saved.\n",
    "\n",
    "shutil.rmtree('base_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5837302e45336cd12f54c7e9737564768e9345dd"
   },
   "source": [
    "Many thanks to HUMERO. \n",
    "\n",
    "Thank you for reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2ff5bf83395fd99eb492e16349df46ee1658a30"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
